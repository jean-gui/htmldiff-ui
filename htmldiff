#!/usr/bin/python3
# -*- coding: utf-8 -*-
# MIT License
# © W3C® (MIT, ERCIM, Keio, Beihang).
# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice (including the next paragraph) shall be included in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

import atexit
import html
import os
import re
import sys
import shutil
import tempfile
import urllib.parse
import urllib.error
from io import StringIO
from subprocess import Popen, PIPE

import http.client
import http_auth
import html5lib
import surbl

CONTENT_TYPE = "text/html;charset=utf-8"

Page = """
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-US">
<head><title>HTML Diff service</title>
<link rel="stylesheet" href="http://www.w3.org/StyleSheets/base" />
</head>
<body>

<p><a href="http://www.w3.org/"><img src="http://www.w3.org/Icons/w3c_home" alt="W3C"/></a> <a href="http://www.w3.org/2003/Editors">W3C Editors homepage</a></p>

<h1>Create Diff between HTML pages</h1>
"""
Page2 = """
<form method="GET">
<p>Address of reference document: <input name="doc1" type="url" value="%s" style="width:100%%"/></p>
<p>Address of new document: <input name="doc2" value="%s"  style="width:100%%"/></p>
<p><input type="submit" value="get Diff"/></p>
</form>

<p><strong>Tip</strong>: if the document uses the W3C convention on linking to its previous version, you can specify only the address of the new document — the previous link will be automatically detected.</p>
<h2>Diff markings</h2>
<p>This service relies on <a href="https://www.gnu.org/software/diffutils/">GNU diff</a>. The found differences are roughly marked as follow:
<ul>
<li>deleted text is shown in pink with down-arrows (as styled for a &lt;del> element)</li>
<li>where there is replacement, it’s shown in green with bi-directional arrows,</li>
<li>where there is newly inserted text, it’s yellow with up arrows (&lt;ins> element)</li>
</ul>
<address>
script $Revision$ of $Date$<br />
by <a href="http://www.w3.org/People/Dom/">Dominique Hazaël-Massieux</a><br />based on <a href="https://github.com/w3c/htmldiff-ui/blob/master/htmldiff.pl">Shane McCarron’ Perl script</a> wrapped in a <a href="https://github.com/w3c/htmldiff-ui/blob/master/htmldiff">Python CGI</a>
</address>
</body>
</html>
"""

class InvalidContentType(Exception):
    "Raised when an HTTP GET returns a non text/html resource"

def validate_url(url):
    "returns true if a url is valid, false otherwise"
    if not url:
        return False

    parsed_url = urllib.parse.urlparse(url)

    # validate schema
    if parsed_url.scheme not in ['http', 'https']:
        return False

    # check domain name has at least two components)
    netloc_parts = parsed_url.netloc.split('.')
    if len(netloc_parts) < 2 or len(netloc_parts[0]) == 0 or len(netloc_parts[1]) == 0:
        return False

    return True

def checkInputUrl(url):
    checker = surbl.SurblChecker('/usr/local/share/surbl/two-level-tlds','/usr/local/etc/surbl.whitelist')

    if  url[:5] == 'file:' or len(urllib.parse.urlparse(url)[0])<2:
        print("Status: 403")
        print("Content-Type: text/plain")
        print()
        print("sorry, I decline to handle file: addresses")
        sys.exit()
    elif not validate_url(url):
        print("Status: 403")
        print("Content-Type: text/plain")
        print()
        url = html.escape(url, True)
        print(f"sorry, I decline to handle this address: {url}")
        sys.exit()
    elif checker.isMarkedAsSpam(url):
        print("Status: 403")
        print("Content-Type: text/plain; charset=utf-8")
        print()
        print("sorry, this URL matches a record known in SURBL. See http://www.surbl.org/")
        sys.exit()

def copyHeader(copy_func, source, key, header_name=None):
    value = source.get(key)
    if not value:
        return False
    elif header_name is None:
        header_name = key
    copy_func(header_name, value)
    return True

def setupRequest():
    opener = http_auth.ProtectedURLopener()
    copyHeader(opener.add_header, os.environ, 'HTTP_IF_MODIFIED_SINCE', 'If-Modified-Since')
    copyHeader(opener.add_header, os.environ, 'REMOTE_ADDR', 'X_Forward_IP_Addr')
    return opener

def processResponse(response):
    """
    Extracts the headers and body returned by a succesful
    DirectorOpener HTTP GET request.

    Stores the body into a temporary file and returns the file and headers.
    Caller must delete the temporary file.
    If there's no response object, returns a (None, {}) tuple.
    """
    if response:

        if not checkResponseIsHTML(response):
            raise InvalidContentType("Content-Type is not text/html")

        tfp = tempfile.NamedTemporaryFile(prefix='htmldiff-', delete=False)
        filename = tfp.name
        # Copy the binary content of the response to the file
        shutil.copyfileobj(response, tfp)
        tfp.close()

        return (filename, response.headers)

    else:
        return (None, {})

def checkResponseIsHTML(response):
    """
    Returns True if headers have a Content-Type header
    and if this header is of type text/html; returns
    False otherwise.
    """
    rv = False

    if 'Content-Type' in response.headers:
        content_type = response.headers.get_content_type()
        if content_type and content_type == 'text/html':
            rv = True

    return rv

def tidyFile(file):
    file.seek(0)
    doc = html5lib.parse(file.read())
    file.close()
    file = tempfile.NamedTemporaryFile(
        mode='w+', prefix='htmldiff-', suffix='.html')
    atexit.register(file.close)
    walker = html5lib.getTreeWalker("etree")
    stream = walker(doc)
    s = html5lib.serializer.HTMLSerializer()
    prettyhtml = s.serialize(stream)
    for item in prettyhtml:
        file.write(item)
    file.flush()
    file.seek(0)
    return (file, [])

def matchPredecessorRel(rel):
    return rel and "predecessor-version" in rel.lower().split(" ")

def rmfile(filename):
    try:
        os.unlink(filename)
    except:
        pass

def escapeURLQueryString(url=None):
    if url is None:
        return None

    url_split = urllib.parse.urlsplit(url)

    if url_split.query:
        fields = dict(urllib.parse.parse_qsl(url_split.query))
        escaped_qs = urllib.parse.urlencode(fields)
        url_split = url_split._replace(query=escaped_qs)
        rv = urllib.parse.urlunsplit(url_split)
    else:
        rv = url

    return rv

def mirrorURL(url, opener):
    try:
        url = escapeURLQueryString(url)
        response = opener.open(url)
        filename, headers = processResponse(response)
    except urllib.error.HTTPError as error:
        opener.error = f"HTTP Error: {error.code} {error.reason}"
    except urllib.error.URLError as error:
        opener.error = f"URL error: {str(error.reason)}"
    except OSError as error:
        opener.error = f"I/O error: {error.errno} {error.strerror}"
    except http.client.InvalidURL:
        opener.error = "Invalid URL submitted"
    except InvalidContentType:
        opener.error = "Content-Type is not text/html"
    except AttributeError:  # ProxyProtectedURLopener returned None.
        pass                # There's already an error set.
    else:
        atexit.register(rmfile, filename)
        file = open(filename)
        if "content-encoding" in headers and headers["content-encoding"] == "gzip":
            import gzip
            data = StringIO(file.read())
            file.close()
            file = gzip.GzipFile(fileobj=data)
        file, errors = tidyFile(file)
        if len(errors) == 0:
            return (file, headers)
        else:
            opener.error = "Tidy errors: %s" % (str(errors))
    return (None, {})

def showPage(url1='', url2='', error_html='', **headers):
    for name, value in list(headers.items()):
        print("%s: %s" % (name.replace('_', '-'), value))
    print()
    print(Page)
    print(error_html)
    print(Page2 % (url1, url2))
    sys.exit()

def serveRequest():

    environ = os.environ
    if 'QUERY_STRING' not in environ:
        showPage(Content_Type=CONTENT_TYPE)

    # Use dict(parse_qsl) so we don't get lists of values.
    fields = dict(urllib.parse.parse_qsl(environ['QUERY_STRING'],
                                         max_num_fields = 2))
    if ('doc2' not in fields):
        showPage(Content_Type=CONTENT_TYPE)
    # if doc1 is not specified, we load doc2 to check if it has a previous version link
    doc2 = fields['doc2']
    checkInputUrl(doc2)
    url_opener2 = setupRequest()
    newdoc, newheaders = mirrorURL(doc2, url_opener2)
    if 'doc1' in fields:
        doc1 = fields['doc1']
    elif newdoc is not None:
        from bs4 import BeautifulSoup

        soup = BeautifulSoup(newdoc.read(), "html.parser")
        newdoc.seek(0)
        try:
            doc1 = soup.find(string=re.compile("Previous Version",re.IGNORECASE)).findNext(name="a", attrs={"href":True})["href"]
        except:
            try:
                doc1 = soup.find(name=["a", "link"], attrs={"href":True, rel:matchPredecessorRel})["href"]
            except:
                doc1 = None
    else:
        doc1 = None
    if (not doc1):
        showPage(Content_Type=CONTENT_TYPE)

    checkInputUrl(doc1)
    esc1 = html.escape(doc1, True)
    esc2 = html.escape(doc2, True)
    urlcomponents1 = urllib.parse.urlparse(doc1)
    urlcomponents2 = urllib.parse.urlparse(doc2)
    # if same domain, we can use the same urlopener
    # otherwise, we create a separate one
    if urlcomponents2[1] == urlcomponents1[1]:
        url_opener = url_opener2
    else:
        url_opener = setupRequest()

    refdoc, refheaders = mirrorURL(doc1, url_opener)
    if not (refdoc and newdoc):
        http_error = ""
        url = ""
        if not refdoc:
            http_error = url_opener.error
            url = esc1
        else:
            http_error = url_opener2.error
            url = esc2
        if re.match("^[1234][0-9][0-9] ", http_error):
            print("Status: %s" %(http_error))
        error="<p style='color:#FF0000'>An error (%s) occured trying to get <a href='%s'>%s</a>.</p>" % (html.escape(http_error), url, url)
        showPage(esc1, esc2, error, Content_Type=CONTENT_TYPE)

    print("Content-Type: text/html")
    if 'Content-Type' in newheaders:
        charset = newheaders.get_content_charset()
        if charset:
            charset = charset.lower()
            #if charset == "iso-8859-1":
            #    options["char_encoding"]='latin1'

    for proxy_header in ('Last-Modified', 'Expires'):
        if copyHeader(lambda header, value: sys.stdout.write("%s: %s" %(header, value)), newheaders, proxy_header):
            print()
    print()
    p = Popen(["/usr/local/bin/htmldiff", refdoc.name, newdoc.name],
              stdin=PIPE, stdout=PIPE, stderr=PIPE)
    sys.stdout.flush()
    sys.stderr.flush()
    (out, err) = p.communicate()
    p.stdin.close()
    if err:
        error = "<p style='color:#FF0000'>An error occured when running <code>htmldiff</code> on the documents:</p><pre>%s</pre>" % (html.escape(err),)
        showPage(esc1, esc2, error)
    else:
        # html5lib strips the doctype, so we add it manually
        # cf https://github.com/w3c/htmldiff-ui/issues/4#issuecomment-642831382
        print("<!doctype html>")
        print(out.decode("utf-8"))
if __name__ == '__main__':
    if 'SCRIPT_NAME' in os.environ:
        serveRequest()
